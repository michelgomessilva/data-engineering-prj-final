from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, trim, upper

from infrastructure.logging.logger import logger


def cleanse_lines_df(spark: SparkSession, input_path: str) -> DataFrame:
    """
    Realiza o cleansing do dataset lines a partir dos dados raw em Parquet.

    Aplica limpeza nas colunas principais, removendo espa√ßos e aplicando
    formata√ß√£o consistente (ex: UPPERCASE para nomes).

    Args:
        spark (SparkSession): Sess√£o Spark ativa.
        input_path (str): Caminho no GCS para o arquivo Parquet de lines.

    Returns:
        DataFrame: DataFrame transformado pronto para ser salvo no BigQuery.
    """
    logger.info(f"üîç Lendo arquivo raw Parquet de: {input_path}")
    df = spark.read.parquet(input_path)

    logger.info("üßπ Limpando e padronizando colunas...")
    cleansed_df = df.select(
        trim(col("line_id")).alias("line_id"),
        upper(trim(col("short_name"))).alias("line_code"),
        upper(trim(col("long_name"))).alias("line_name"),
        trim(col("color")).alias("color"),
        trim(col("text_color")).alias("text_color"),
        col("localities"),
        col("municipalities"),
        col("routes"),
        col("patterns"),
        col("facilities"),
        col("date"),
    ).dropDuplicates(["line_id"])

    logger.success("‚úÖ Cleansing do lines conclu√≠do.")
    return cleansed_df
