
# üìä Relat√≥rio T√©cnico: Projeto final Curso Data Engineering

## 1. Introdu√ß√£o

Este projeto final de Data Engineering
tem como objetivo construir uma pipeline robusta e escal√°vel para ingest√£o,
transforma√ß√£o e carregamento de dados, simulando um processo de batching.
Os dados ingeridos prov√™m da API da Carris, s√£o carregados no BigQuery atrav√©s do PySpark e,
posteriormente, s√£o transformados com recurso ao dbt
O prop√≥sito principal √© garantir a qualidade, integridade e acessibilidade dos dados para suportar an√°lises
e tomadas de decis√£o informadas.
Todo este processo ELT ser√° orquestrado em diferentes etapas com recurso ao Apache Airflow.

## 2. Arquitetura da Solu√ß√£o

A arquitetura adotada neste projeto segue os princ√≠pios de processamento batch e ELT,
garantindo escalabilidade, modularidade e resili√™ncia.

**Componentes principais:**
- **Fontes de dados**: *API da Carris*
- **Ingest√£o**: *PySpark*
- **Processamento e transforma√ß√£o**: *(PySpark e dbt)*
- **Armazenamento**: *Google Cloud Storage Bucket (Raw Layer) e BigQuery (Staging Layer e Mart Layer)*
- **Orquestra√ß√£o**: *Apache Airflow*

	- Cada pipeline √© representado por um DAG.
	- DAGs controlam:
	  1. Extra√ß√£o de dados e grava√ß√£o no GCS
	  2. Carregamento para BigQuery (staging)
	  3. Transforma√ß√µes e cria√ß√£o de tabelas mart
	- Reprocessamento por `execution_date`, logging, alertas e monitoriza√ß√£o pela UI.

- **Destino final (consumo)**: *BigQuery - Marts Layer* -- Apagar??

> üîÅ Sugest√£o: incluir um diagrama de arquitetura aqui (ex: em formato imagem ou PlantUML - imagens/arquitectura.png)

## 3. Modelo de Dados Final

O modelo de dados foi desenhado para suportar an√°lises de neg√≥cio com foco em performance e clareza. Utiliz√°mos um esquema em estrela composto por:

- **Tabelas Fato**: *(ex: fact_sales, fact_orders)*
- **Tabelas Dimens√£o**: *(ex: dim_customers, dim_products, dim_date)*

**Exemplo de estrutura da tabela `fact_sales`:**
| Campo           | Tipo     | Descri√ß√£o                    |
|----------------|----------|------------------------------|
| sale_id         | INT      | Identificador √∫nico da venda |
| customer_id     | INT      | FK para `dim_customers`      |
| product_id      | INT      | FK para `dim_products`       |
| sale_date       | DATE     | Data da venda                |
| amount          | FLOAT    | Valor da venda               |

> üìê Sugest√£o: adicionar aqui um diagrama ER ou modelo estrela.

## 4. Pipeline de Dados: Extract Load and Transform (ETL/ELT)

### 4.1 Data Ingestion

- **Fontes**: *API da Carris - Fornece informa√ß√£o detalhada √† cerca de linhas, rotas, paragens, hor√°rios, entre outros*
- **Frequ√™ncia**: *di√°ria, em batch*
- **Ferramentas utilizadas**: *Spark, GCS*

#### 4.1.1 Endpoints utilizados

Tendo em conta a informa√ß√£o a analisar relativa a Stops e Trips, decidimos carregar informa√ß√µes dos seguintes endpoints:

##### 4.1.1.1 gtfs

##### 4.1.1.2 vehicles

	- Devolve informa√ß√£o sobre todos os ve√≠culos operados pela carris, nomeadamente a √∫ltima localiza√ß√£o conhecida

	Exemplo resposta:
	{
        id: "41|1153",
        lat: 38.740165,
        lon: -9.268897,
        speed: 0,
        heading: 68.0999984741211,
        trip_id: "1724_0_2_2030_2059_0_7",
        pattern_id: "1724_0_2"
        timestamp: 1693948520000,
    }

##### 4.1.1.3 municipalities
	- Devolve informa√ß√£o sobre os munic√≠pios na zona metropolitana de Lisboa, bem como a √°rea adjacente onde a Carris disponibiliza os seus servi√ßos

	Exemplo resposta:
    {
        id: "1502",
        name: "Alcochete",
        prefix: "01",
        district_id: "15",
        district_name: "Set√∫bal",
        region_id: "PT170",
        region_name: "AML",
    }


##### 4.1.1.4 stops

##### 4.1.1.5 lines

##### 4.1.1.6 routes

##### 4.1.1.7 datasets/facilities

### 4.2 Data Cleansing

- Remo√ß√£o de duplicados e nulos
- Padroniza√ß√£o de campos (datas, texto, c√≥digos)
- Valida√ß√£o de chaves estrangeiras

### 4.3 Data Modelling

- Agrega√ß√µes (ex: vendas por dia, cliente, regi√£o)
- Cria√ß√£o de campos derivados (ex: ano-m√™s, flags)
- Jun√ß√µes entre entidades
- Normaliza√ß√£o e desnormaliza√ß√£o quando necess√°rio

## 5. Decis√µes T√©cnicas e Justifica√ß√µes

**Principais decis√µes tomadas:**

- **Estrutura em camadas bronze/silver/gold**: facilita rastreabilidade e reprocessamento.
- **Uso de PySpark** para transformar dados em larga escala devido √† sua performance distribu√≠da.
- **Armazenamento em formato Parquet** pelas vantagens de compress√£o e leitura otimizada.
- **Particionamento por data** para melhorar a performance de leitura em queries anal√≠ticas.
- **Orquestra√ß√£o com Airflow** para maior controlo e monitoriza√ß√£o da pipeline.

> üß† Cada decis√£o foi tomada considerando um equil√≠brio entre performance, custo, simplicidade e manuten√ß√£o futura atrav√©s do recurso a parametriza√ß√µes e modula√ß√£o do c√≥digo.

## 6. Considera√ß√µes Finais

O projeto atingiu os objetivos iniciais de cria√ß√£o de uma pipeline confi√°vel e eficiente. O modelo de dados permite an√°lise r√°pida e precisa. Como pr√≥ximos passos, sugere-se:

- Implementa√ß√£o de testes automatizados de dados
- Monitoriza√ß√£o ativa de qualidade e lat√™ncia
- Expans√£o para novas fontes e integra√ß√£o com ferramentas de BI


\* _Adaptar os termos e exemplos entre par√™ntesis ao teu caso espec√≠fico._
