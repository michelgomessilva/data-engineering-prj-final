"""
IngestStopsService

Classe respons√°vel por orquestrar o processo de ingest√£o de dados brutos (raw)
de stops da API da Carris Metropolitana,
normalizar os dados, convert√™-los em DataFrame do Spark, e armazen√°-los em
formato Parquet no Google Cloud Storage (GCS),
organizados em parti√ß√µes di√°rias.

Responsabilidades:
- Buscar os dados da API externa definida no settings (endpoint de paragens).
- Normalizar os dados JSON em um formato tabular.
- Converter para Spark DataFrame.
- Adicionar metadados como a data de ingest√£o.
- Salvar os dados particionados por data no bucket do GCS, na camada raw.

Atributos:
- self.api: inst√¢ncia da StopsCarrisAPI (para chamadas HTTP).
- self.spark: sess√£o Spark obtida pela fun√ß√£o `get_spark_session`.
- self.repo: respons√°vel por converter os dados normalizados em DataFrame.
- self.storage: respons√°vel por salvar o DataFrame no storage (Parquet + GCS).

M√©todo principal:
- ingest(): executa todo o pipeline de ingest√£o raw.
"""

from pyspark.sql.functions import current_date

from configs.settings import Settings
from domain.normalizers.stops_normalizer import StopsNormalizer
from domain.schemas.stops_schema import stops_schema
from domain.services.base_ingestion_services import IBaseIngestService
from infrastructure.api.carris_client import CarrisAPIClient
from infrastructure.logging.logger import logger
from infrastructure.repositories.generic_spark_repository import GenericSparkRepository
from infrastructure.spark.create_session_spark import get_spark_session
from infrastructure.storage.parquet_storage import ParquetStorage


class IngestStopsService(IBaseIngestService):
    def __init__(self):
        logger.info("Inicializando IngestStopsService...")

        self.api = CarrisAPIClient()
        logger.debug("Cliente da API Carris inicializado.")

        self.spark = get_spark_session()
        logger.debug("Sess√£o Spark obtida com sucesso.")

        self.repo = GenericSparkRepository(self.spark, schema=stops_schema)
        logger.debug("Reposit√≥rio gen√©rico com schema de stops criado.")

        self.storage = ParquetStorage()
        logger.debug("Servi√ßo de armazenamento Parquet instanciado.")

    def ingest(self):
        logger.info("Iniciando pipeline de ingest√£o de stops...")

        logger.info(f"Buscando dados do endpoint: {Settings.STOPS_ENDPOINT}")
        raw_data = self.api.fetch(Settings.STOPS_ENDPOINT)
        logger.success(f"{len(raw_data)} registros brutos recebidos da API.")

        logger.info("Normalizando dados brutos...")
        normalized = StopsNormalizer.normalize(raw_data)
        logger.success(f"Normaliza√ß√£o conclu√≠da. Total de registros: {len(normalized)}")

        logger.info("üß™ Convertendo para DataFrame do Spark...")
        df = self.repo.to_dataframe(normalized)
        logger.debug("Esquema do DataFrame:")
        df.printSchema()

        # Adiciona a coluna 'date' para particionamento por data
        df = df.withColumn("date", current_date())
        logger.debug("Coluna 'date' adicionada ao DataFrame.")

        # Define o caminho de destino no bucket
        logger.info("Salvando dados no GCS particionados por data...")
        gcs_path = Settings.get_raw_path(Settings.STOPS_ENDPOINT)

        # Salva o DataFrame no GCS particionado por data
        logger.info(f"Salvando DataFrame no GCS: {gcs_path}")
        self.storage.save(
            df, gcs_path, mode="overwrite", partition_by=["date"], coalesce=1
        )
        logger.success("Dados de stops salvos com sucesso no GCS!")


def run_ingest_stops():
    """
    Fun√ß√£o utilizada como ponto de entrada para execu√ß√£o do pipeline de ingest√£o
    de stops. Ela instancia o servi√ßo `IngestStopsService` e executa o m√©todo
    `ingest()`.

    Essa fun√ß√£o √© usada diretamente como `python_callable` na DAG do Airflow.
    """
    logger.info("Iniciando use case: ingest_stops")
    service = IngestStopsService()
    service.ingest()
    logger.success("Use case 'ingest_stops' finalizado com sucesso.")
