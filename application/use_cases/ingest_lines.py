"""
IngestLinesService

Classe respons√°vel por orquestrar o processo de ingest√£o de dados brutos (raw)
de ve√≠culos da API da Carris Metropolitana,
normalizar os dados, convert√™-los em DataFrame do Spark, e armazen√°-los em
formato Parquet no Google Cloud Storage (GCS),
organizados em parti√ß√µes di√°rias.

Responsabilidades:
- Buscar os dados da API externa definida no settings (endpoint de lines).
- Normalizar os dados JSON em um formato tabular.
- Converter para Spark DataFrame.
- Adicionar metadados como a data de ingest√£o.
- Salvar os dados particionados por data no bucket do GCS, na camada raw.

Atributos:
- self.api: inst√¢ncia da LinesCarrisAPI (para chamadas HTTP).
- self.spark: sess√£o Spark obtida pela fun√ß√£o `get_spark_session`.
- self.repo: respons√°vel por converter os dados normalizados em DataFrame.
- self.storage: respons√°vel por salvar o DataFrame no storage (Parquet + GCS).

M√©todo principal:
- ingest(): executa todo o pipeline de ingest√£o raw.
"""

from pyspark.sql.functions import current_date

from configs.settings import Settings
from domain.normalizers.lines_normalizer import LinesNormalizer
from domain.schemas.lines_schema import lines_schema
from domain.services.base_ingestion_services import IBaseIngestService
from infrastructure.api.carris_client import CarrisAPIClient
from infrastructure.logging.logger import logger
from infrastructure.repositories.generic_spark_repository import GenericSparkRepository
from infrastructure.spark.spark_singleton import get_spark_session
from infrastructure.storage.parquet_storage import ParquetStorage


class IngestLinesService(IBaseIngestService):
    def __init__(self):
        logger.info("Inicializando IngestLinesService...")

        self.api = CarrisAPIClient()
        logger.debug("Cliente da API Carris inicializado.")

        self.spark = get_spark_session()
        logger.debug("Sess√£o Spark obtida com sucesso.")

        self.repo = GenericSparkRepository(self.spark, schema=lines_schema)
        logger.debug("Reposit√≥rio gen√©rico com schema de linhas criado.")

        self.storage = ParquetStorage()
        logger.debug("Servi√ßo de armazenamento Parquet instanciado.")

    def ingest(self):
        logger.info("Iniciando pipeline de ingest√£o de linhas...")

        logger.info(f"Buscando dados do endpoint: {Settings.LINES_ENDPOINT}")
        raw_data = self.api.fetch(Settings.LINES_ENDPOINT)
        logger.success(f"{len(raw_data)} registros brutos recebidos da API.")

        logger.info("Normalizando dados brutos...")
        normalized = LinesNormalizer.normalize(raw_data)
        logger.success(f"Normaliza√ß√£o conclu√≠da. Total de registros: {len(normalized)}")

        logger.info("üß™ Convertendo para DataFrame do Spark...")
        df = self.repo.to_dataframe(normalized)
        logger.debug("Esquema do DataFrame:")
        df.printSchema()

        # Adiciona a coluna 'date' para particionamento por data
        df = df.withColumn("date", current_date())
        logger.debug("Coluna 'date' adicionada ao DataFrame.")

        # Ajusta o n√∫mero de parti√ß√µes com base no tamanho do DataFrame
        num_rows = df.count()
        coalesce = 1
        if num_rows < 10_000:
            coalesce = 1
        elif num_rows < 100_000:
            coalesce = 4
        else:
            df = df.repartition("date")

        # Define o caminho de destino no bucket
        logger.info("Salvando dados no GCS particionados por data...")
        gcs_path = Settings.get_raw_path(Settings.LINES_ENDPOINT)

        # Salva o DataFrame no GCS particionado por data
        logger.info(f"Salvando DataFrame no GCS: {gcs_path}")
        self.storage.save(df, gcs_path, mode="overwrite", coalesce=coalesce)
        logger.success("Dados de lines salvos com sucesso no GCS!")


def run_ingest_lines():
    """
    Fun√ß√£o utilizada como ponto de entrada para execu√ß√£o do pipeline de ingest√£o
    de linhas. Ela instancia o servi√ßo `IngestLinesService` e executa o m√©todo
    `ingest()`.

    Essa fun√ß√£o √© usada diretamente como `python_callable` na DAG do Airflow.
    """
    logger.info("Iniciando use case: ingest_lines")
    service = IngestLinesService()
    service.ingest()
    logger.success("Use case 'ingest_lines' finalizado com sucesso.")
